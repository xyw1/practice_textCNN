{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata \n",
    "from mxnet.gluon import loss as gloss \n",
    "from mxnet.gluon import utils as gutils\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, init, nd\n",
    "import pickle\n",
    "import d2lzh as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectional 设为True即得到双向循环网络\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "                                   bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        # inputs的形状是（批量大小， 词数），因为LSTM需要将序列作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为（词数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # rnn.LSTM只传入输入embeddings, 因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是（词数，批量大小，2*隐藏单元个数）\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # （批量大小，4*隐藏单元个数）\n",
    "        encoding = nd.concat(outputs[0], outputs[-1])\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = d2l.read_imdb('train'),d2l.read_imdb('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = pickle.load(open('../pkl/glove_embedding.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = d2l.get_vocab_imdb(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(*d2l.preprocess_imdb(train_data,vocab))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_set = gdata.ArrayDataset(*d2l.preprocess_imdb(test_data,vocab))\n",
    "test_iter = gdata.DataLoader(test_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, ctx = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "net.embedding.collect_params().setattr('grad_req','null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [cpu(0)]\n",
      "epoch 1, loss 0.5172, train acc 0.735, test acc 0.821, time 3105.5 sec\n",
      "epoch 2, loss 0.3971, train acc 0.827, test acc 0.834, time 2931.8 sec\n",
      "epoch 3, loss 0.3545, train acc 0.851, test acc 0.845, time 2724.7 sec\n",
      "epoch 4, loss 0.3204, train acc 0.868, test acc 0.834, time 1737.8 sec\n",
      "epoch 5, loss 0.2875, train acc 0.884, test acc 0.844, time 1423.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
